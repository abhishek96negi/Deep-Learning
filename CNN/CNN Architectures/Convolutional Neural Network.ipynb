{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What are Convolutional Neural Networks?\n",
    "\n",
    "Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.\n",
    "\n",
    "\n",
    "A Convolutional Neural Network (CNN) is comprised of one or more convolutional layers (often with a subsampling step) and then followed by one or more fully connected layers as in a standard multilayer neural network. The architecture of a CNN is designed to take advantage of the 2D structure of an input image (or other 2D input such as a speech signal). This is achieved with local connections and tied weights followed by some form of pooling which results in translation invariant features. Another benefit of CNNs is that they are easier to train and have many fewer parameters than fully connected networks with the same number of hidden units. In this article we will discuss the architecture of a CNN and the back propagation algorithm to compute the gradient with respect to the parameters of the model in order to use gradient based optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's develop better intuition for how Convolutional Neural Networks (CNN) work. We'll examine how humans classify images, and then see how CNNs use similar approaches.\n",
    "\n",
    "Let’s say we wanted to classify the following image of a dog as a Golden Retriever:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377b77_dog-1210559-1280/dog-1210559-1280.jpg\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As humans, how do we do this?\n",
    "\n",
    "One thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur. We essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog.\n",
    "\n",
    "In this case, we might break down the image into a combination of the following:\n",
    "\n",
    "* A nose\n",
    "* Two eyes\n",
    "* Golden fur\n",
    "\n",
    "These pieces can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377bdb_screen-shot-2016-11-24-at-12.49.08-pm/screen-shot-2016-11-24-at-12.49.08-pm.png\" width=\"250\" height=\"250\">\n",
    "<center>The eye of the dog.</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377bed_screen-shot-2016-11-24-at-12.49.43-pm/screen-shot-2016-11-24-at-12.49.43-pm.png\" width=\"250\" height=\"250\">\n",
    "<center>The nose of the dog.</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377bff_screen-shot-2016-11-24-at-12.50.54-pm/screen-shot-2016-11-24-at-12.50.54-pm.png\" width=\"250\" height=\"250\">\n",
    "<center>The fur of the dog.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going One Step Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let’s take this one step further. How do we determine what exactly a nose is? A Golden Retriever nose can be seen as an oval with two black holes inside it. Thus, one way of classifying a Retriever’s nose is to to break it up into smaller pieces and look for black holes (nostrils) and curves that define an oval as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377c52_screen-shot-2016-11-24-at-12.51.47-pm/screen-shot-2016-11-24-at-12.51.47-pm.png\">\n",
    "<center>A curve that we can use to determine a nose</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377c68_screen-shot-2016-11-24-at-12.51.51-pm/screen-shot-2016-11-24-at-12.51.51-pm.png\">\n",
    "<center>A nostril that we can use to classify the nose of the dog</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, this is what a CNN learns to do. It learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image. Finally, the CNN classifies the image by combining the larger, more complex objects.\n",
    "\n",
    "In our case, the levels in the hierarchy are:\n",
    "\n",
    "* Simple shapes, like ovals and dark circles\n",
    "* Complex objects (combinations of simple shapes), like eyes, nose, and fur\n",
    "* The dog as a whole (a combination of complex objects)\n",
    "\n",
    "With deep learning, we don't actually program the CNN to recognize these specific features. Rather, the CNN learns on its own to recognize such objects through forward propagation and backpropagation!\n",
    "\n",
    "It's amazing how well a CNN can learn to classify images, even though we never program the CNN with information about specific features to look for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/583cb19d_heirarchy-diagram/heirarchy-diagram.jpg)\n",
    "<center>An example of what each layer in a CNN might recognize when classifying a picture of a dog</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects. The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors. The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs.\n",
    "\n",
    "Once again, the CNN **learns all of this on its own**. We don't ever have to tell the CNN to go looking for lines or curves or noses or fur. The CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking up an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for a CNN is to break up the image into smaller pieces. We do this by selecting a width and height that defines a filter.\n",
    "\n",
    "The filter looks at small pieces, or patches, of the image. These patches are the same size as the filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377d67_vlcsnap-2016-11-24-15h52m47s438/vlcsnap-2016-11-24-15h52m47s438.png\" width=600 height=400>\n",
    "<center>A CNN uses filters to split an image into smaller patches. The size of these patches matches the filter size.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then simply slide this filter horizontally or vertically to focus on a different piece of the image.\n",
    "\n",
    "The amount by which the filter slides is referred to as the **'stride'**. The stride is a hyperparameter which the engineer can tune. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy.\n",
    "\n",
    "Let’s look at an example. In this zoomed in image of the dog, we first start with the patch outlined in red. The width and height of our filter define the size of this square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/December/5840fdac_retriever-patch/retriever-patch.png\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move the square over to the right by a given stride (2 in this case) to get another patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/December/5840fe04_retriever-patch-shifted/retriever-patch-shifted.png\" width=500 height=500>\n",
    "<center>We move our square to the right by two pixels to create another patch.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's important here is that we are **grouping together adjacent pixels** and treating them as a collective.\n",
    "\n",
    "In a normal, non-convolutional neural network, we would have ignored this adjacency. In a normal network, we would have connected every pixel in the input image to a neuron in the next layer. In doing so, we would not have taken advantage of the fact that pixels in an image are close together for a reason and have special meaning.\n",
    "\n",
    "By taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to have more than one filter. Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. The amount of filters in a convolutional layer is called the **filter depth**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377e4f_neilsen-pic/neilsen-pic.png)\n",
    "<center>In the above example, a patch is connected to a neuron in the next layer. Source: MIchael Neilsen.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many neurons does each patch connect to?\n",
    "\n",
    "That’s dependent on our filter depth. If we have a depth of `k`, we connect each patch of pixels to `k` neurons in the next layer. This gives us the height of `k` in the next layer, as shown below. In practice, `k` is a hyperparameter we tune, and most CNNs tend to pick the same starting values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/December/5840ffda_filter-depth/filter-depth.png\" width=\"300\" height=\"500\">\n",
    "<center>Choosing a filter depth of k connects each path to k neurons in the next layer</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why connect a single patch to multiple neurons in the next layer? Isn’t one neuron good enough?\n",
    "\n",
    "Multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture.\n",
    "\n",
    "For example, one patch might include some white teeth, some blonde whiskers, and part of a red tongue. In that case, we might want a filter depth of at least three - one for each of teeth, whiskers, and tongue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584104c8_teeth-whiskers-tongue/teeth-whiskers-tongue.png\" width=\"350\" height=\"400\">\n",
    "<center>This patch of the dog has many interesting features we may want to capture. These include the presence of teeth, the presence of whiskers, and the pink color of the tongue.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having multiple neurons for a given patch ensures that our CNN can learn to capture whatever characteristics the CNN learns are important.\n",
    "\n",
    "Remember that the CNN isn't \"programmed\" to look for certain characteristics. Rather, it **learns on its own** which characteristics to notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377f77_vlcsnap-2016-11-24-16h01m35s262/vlcsnap-2016-11-24-16h01m35s262.png\" width=600 height=400>\n",
    "<center>The weights, w, are shared across patches for a given layer in a CNN to detect the cat above regardless of where in the image it is located.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are trying to classify a picture of a cat, we don’t care where in the image a cat is. If it’s in the top left or the bottom right, it’s still a cat in our eyes. We would like our CNNs to also possess this ability known as translation invariance. How can we achieve this?\n",
    "\n",
    "As we saw earlier, the classification of a given patch in an image is determined by the weights and biases corresponding to that patch.\n",
    "\n",
    "If we want a cat that’s in the top left patch to be classified in the same way as a cat in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way.\n",
    "\n",
    "This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "\n",
    "There’s an additional benefit to sharing our parameters. If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images. Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5837d4d5_screen-shot-2016-11-24-at-10.05.37-pm/screen-shot-2016-11-24-at-10.05.37-pm.png)\n",
    "<center>A 5x5 grid with a 3x3 filter. Source: Andrej Karpathy.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a `5x5` grid (as shown above) and a filter of size `3x3` with a stride of `1`. What's the width and height of the next layer? We see that we can fit at most three patches in each direction, giving us a dimension of `3x3` in our next layer. As we can see, the width and height of each subsequent layer decreases in such a scheme.\n",
    "\n",
    "In an ideal world, we'd be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency. How might we achieve this? One way is to simple add a border of `0`s to our original `5x5` image. You can see what this looks like in the below image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5837d4ee_screen-shot-2016-11-24-at-10.05.46-pm/screen-shot-2016-11-24-at-10.05.46-pm.png)\n",
    "<center>The same grid with 0 padding. Source: Andrej Karpathy.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would expand our original image to a `7x7`. With this, we now see how our next layer's size is again a `5x5`, keeping our dimensionality consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we've learned so far, how can we calculate the number of neurons of each layer in our CNN?\n",
    "\n",
    "Given our input layer has a volume of `W`, our filter has a volume `(height * width * depth)` of `F`, we have a stride of `S`, and a padding of `P`, the following formula gives us the volume of the next layer: `(W−F+2P)/S+1`.\n",
    "\n",
    "Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at an example CNN to see how it works in action.\n",
    "\n",
    "The CNN we will look at is trained on ImageNet as described in [this paper](http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf) by Zeiler and Fergus. In the images below (from the same paper), we’ll see what each layer in this network detects and see *how* each layer detects more and more complex ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/583cbd42_layer-1-grid/layer-1-grid.png)\n",
    "<center>Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle).</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images above are from Matthew Zeiler and Rob Fergus' [deep visualization toolbox](https://www.youtube.com/watch?v=ghEmQSxT6tw), which lets us visualize what each layer in a CNN focuses on.\n",
    "\n",
    "Each image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. These squares are shown below again for reference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/583cbba2_diagonal-line-1/diagonal-line-1.png)\n",
    "<center>As visualized here, the first layer of the CNN can recognize -45 degree lines.</center>\n",
    "<br>\n",
    "\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/583cbc02_diagonal-line-2/diagonal-line-2.png)\n",
    "<center>The first layer of the CNN is also able to recognize +45 degree lines, like the one above.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/583cbace_grid-layer-1/grid-layer-1.png)\n",
    "<center>Example patches that activate the -45 degree line detector in the first layer.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/583780f3_screen-shot-2016-11-24-at-12.09.02-pm/screen-shot-2016-11-24-at-12.09.02-pm.png\" width=700 height=700>\n",
    "<center>A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layer of the CNN captures complex ideas.\n",
    "\n",
    "As you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right).\n",
    "\n",
    "**The CNN learns to do this on its own**. There is no special instruction for the CNN to focus on more complex objects in deeper layers. That's just how it normally works out when you feed training data into a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5837811f_screen-shot-2016-11-24-at-12.09.24-pm/screen-shot-2016-11-24-at-12.09.24-pm.png\" width=700 height=700>\n",
    "<center>A visualization of the third layer in the CNN. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58378151_screen-shot-2016-11-24-at-12.08.11-pm/screen-shot-2016-11-24-at-12.08.11-pm.png\" width=500 height=500>\n",
    "<center>A visualization of the fifth and final layer of the CNN. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN.\n",
    "\n",
    "The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how to implement a CNN in TensorFlow.\n",
    "\n",
    "TensorFlow provides the `tf.nn.conv2d()` and `tf.nn.bias_add()` functions to create your own convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses the `tf.nn.conv2d()` function to compute the convolution with `weight` as the filter and `[1, 2, 2, 1]` for the strides. TensorFlow uses a stride for each `input` dimension, `[batch, input_height, input_width, input_channels]`. We are generally always going to set the stride for `batch` and `input_channels `(i.e. the first and fourth element in the strides array) to be `1`.\n",
    "\n",
    "You'll focus on changing `input_height` and `input_width` while setting batch and `input_channels` to `1`. The `input_height` and `input_width` strides are for striding the filter over `input`. This example code uses a stride of 2 with 5x5 filter over `input`.\n",
    "\n",
    "The `tf.nn.bias_add()` function adds a 1-d bias to the last dimension in a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Covnet-ology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/582aac09_max-pooling/max-pooling.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above is an example of max pooling with a 2x2 filter and stride of 2. The four 2x2 colors represent each time the filter was applied to find the maximum value.\n",
    "\n",
    "For example, `[[1, 0], [4, 6]]` becomes `6`, because `6` is the maximum value in this set. Similarly, `[[2, 3], [6, 8]]` becomes `8`.\n",
    "\n",
    "Conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values.\n",
    "\n",
    "TensorFlow provides the `tf.nn.max_pool()` function to apply max pooling to your convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.max_pool()` function performs max pooling with the `ksize` parameter as the size of the filter and the `strides` parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice.\n",
    "\n",
    "The `ksize` and `strides` parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor (`[batch, height, width, channels]`). For both `ksize` and `strides`, the `batch` and `channel` dimensions are typically set to `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1x1 Convolutions Video](https://www.youtube.com/watch?v=Zmzgerm6SjA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Inception Module Video](https://www.youtube.com/watch?v=SlTm03bEOxA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to walk through an example Convolutional Neural Network (CNN) in TensorFlow.\n",
    "\n",
    "The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers.\n",
    "\n",
    "The code we'll be looking at is similar to what we saw in the segment on Deep Neural Network in TensorFlow, except we'll restructured the architecture of this network as a CNN.\n",
    "\n",
    "Just like in that segment, here we'll study the line-by-line breakdown of the code. [Link to download the code and run it.](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a61ca1_cnn/cnn.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen this section of code from previous lessons. Here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581a58be_convolution-schematic/convolution-schematic.gif)\n",
    "<centre>Convolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution</centre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is an example of a convolution with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, `[[1, 0, 1], [0, 1, 0], [1, 0, 1]]`, then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using `tf.nn.conv2d()` and `tf.nn.bias_add()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.conv2d()` function computes the convolution against weight `W` as shown above.\n",
    "\n",
    "In TensorFlow, stride is an array of 4 elements; the first element in the stride array indicates the stride for batch and last element indicates stride for features. It's good practice to remove the batches or features you want to skip from the data set rather than use stride to skip them. You can always set the first and last element to 1 in stride in order to use all batches and features.\n",
    "\n",
    "The middle two elements are the strides for height and width respectively. I've mentioned stride as one number because you usually have a square stride where `height = width`. When someone says they are using a stride of 3, they usually mean `tf.nn.conv2d(x, W, strides=[1, 3, 3, 1])`.\n",
    "\n",
    "To make life easier, the code is using `tf.nn.bias_add()` to add the bias. Using `tf.add()` doesn't work when the tensors aren't the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581a57fe_maxpool/maxpool.jpeg)\n",
    "<centre>Max Pooling with 2x2 filter and stride of 2. Source: http://cs231n.github.io/convolutional-networks/</centre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is an example of max pooling with a 2x2 filter and stride of 2. The left square is the input and the right square is the output. The four 2x2 colors in input represents each time the filter was applied to create the max on the right side. For example, `[[1, 1], [5, 6]]` becomes `6` and `[[3, 2], [1, 2]]` becomes `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.max_pool()` function does exactly what you would expect, it performs max pooling with the `ksize` parameter as the size of the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581a64b7_arch/arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we're creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step. Then next step applies max pooling, turning each sample into 14x14x32. All the layers are applied from `conv1` to `output`, producing 10 class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-832e87651e29>:14: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-10-d092b27ddbc1>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch  1, Batch   1 -Loss: 62693.5078 Validation Accuracy: 0.035156\n",
      "Epoch  1, Batch   2 -Loss: 55347.6992 Validation Accuracy: 0.046875\n",
      "Epoch  1, Batch   3 -Loss: 52069.5078 Validation Accuracy: 0.054688\n",
      "Epoch  1, Batch   4 -Loss: 41663.2383 Validation Accuracy: 0.070312\n",
      "Epoch  1, Batch   5 -Loss: 37907.5312 Validation Accuracy: 0.074219\n",
      "Epoch  1, Batch   6 -Loss: 32102.1777 Validation Accuracy: 0.074219\n",
      "Epoch  1, Batch   7 -Loss: 31752.9453 Validation Accuracy: 0.078125\n",
      "Epoch  1, Batch   8 -Loss: 27729.6816 Validation Accuracy: 0.078125\n",
      "Epoch  1, Batch   9 -Loss: 23322.7344 Validation Accuracy: 0.074219\n",
      "Epoch  1, Batch  10 -Loss: 25012.4141 Validation Accuracy: 0.085938\n",
      "Epoch  1, Batch  11 -Loss: 23341.8633 Validation Accuracy: 0.089844\n",
      "Epoch  1, Batch  12 -Loss: 23156.8555 Validation Accuracy: 0.089844\n",
      "Epoch  1, Batch  13 -Loss: 23120.0820 Validation Accuracy: 0.101562\n",
      "Epoch  1, Batch  14 -Loss: 22169.7852 Validation Accuracy: 0.117188\n",
      "Epoch  1, Batch  15 -Loss: 20562.4316 Validation Accuracy: 0.121094\n",
      "Epoch  1, Batch  16 -Loss: 19483.7773 Validation Accuracy: 0.136719\n",
      "Epoch  1, Batch  17 -Loss: 18610.1426 Validation Accuracy: 0.167969\n",
      "Epoch  1, Batch  18 -Loss: 18546.6172 Validation Accuracy: 0.164062\n",
      "Epoch  1, Batch  19 -Loss: 15921.4707 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  20 -Loss: 17949.2109 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  21 -Loss: 16504.8730 Validation Accuracy: 0.191406\n",
      "Epoch  1, Batch  22 -Loss: 15281.3271 Validation Accuracy: 0.203125\n",
      "Epoch  1, Batch  23 -Loss: 16232.7305 Validation Accuracy: 0.203125\n",
      "Epoch  1, Batch  24 -Loss: 15565.7881 Validation Accuracy: 0.226562\n",
      "Epoch  1, Batch  25 -Loss: 15926.3682 Validation Accuracy: 0.234375\n",
      "Epoch  1, Batch  26 -Loss: 15355.2422 Validation Accuracy: 0.234375\n",
      "Epoch  1, Batch  27 -Loss: 16053.1074 Validation Accuracy: 0.230469\n",
      "Epoch  1, Batch  28 -Loss: 15568.5107 Validation Accuracy: 0.207031\n",
      "Epoch  1, Batch  29 -Loss: 14270.6787 Validation Accuracy: 0.207031\n",
      "Epoch  1, Batch  30 -Loss: 13292.3125 Validation Accuracy: 0.210938\n",
      "Epoch  1, Batch  31 -Loss: 12520.4189 Validation Accuracy: 0.222656\n",
      "Epoch  1, Batch  32 -Loss: 13929.2686 Validation Accuracy: 0.234375\n",
      "Epoch  1, Batch  33 -Loss: 12165.7598 Validation Accuracy: 0.234375\n",
      "Epoch  1, Batch  34 -Loss: 13385.1221 Validation Accuracy: 0.230469\n",
      "Epoch  1, Batch  35 -Loss: 11222.6289 Validation Accuracy: 0.230469\n",
      "Epoch  1, Batch  36 -Loss: 11709.3096 Validation Accuracy: 0.242188\n",
      "Epoch  1, Batch  37 -Loss: 10023.8809 Validation Accuracy: 0.257812\n",
      "Epoch  1, Batch  38 -Loss: 14301.8770 Validation Accuracy: 0.273438\n",
      "Epoch  1, Batch  39 -Loss: 10946.4346 Validation Accuracy: 0.289062\n",
      "Epoch  1, Batch  40 -Loss:  9728.4834 Validation Accuracy: 0.296875\n",
      "Epoch  1, Batch  41 -Loss: 10608.8965 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  42 -Loss:  9989.1631 Validation Accuracy: 0.289062\n",
      "Epoch  1, Batch  43 -Loss:  9629.2061 Validation Accuracy: 0.292969\n",
      "Epoch  1, Batch  44 -Loss: 10037.4893 Validation Accuracy: 0.300781\n",
      "Epoch  1, Batch  45 -Loss:  9992.0449 Validation Accuracy: 0.296875\n",
      "Epoch  1, Batch  46 -Loss: 10473.2832 Validation Accuracy: 0.316406\n",
      "Epoch  1, Batch  47 -Loss:  9294.7930 Validation Accuracy: 0.328125\n",
      "Epoch  1, Batch  48 -Loss:  8927.1133 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  49 -Loss: 11209.3711 Validation Accuracy: 0.316406\n",
      "Epoch  1, Batch  50 -Loss:  8761.5674 Validation Accuracy: 0.328125\n",
      "Epoch  1, Batch  51 -Loss:  8782.1328 Validation Accuracy: 0.328125\n",
      "Epoch  1, Batch  52 -Loss:  7771.1128 Validation Accuracy: 0.332031\n",
      "Epoch  1, Batch  53 -Loss:  9352.3457 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  54 -Loss:  8196.9873 Validation Accuracy: 0.343750\n",
      "Epoch  1, Batch  55 -Loss:  8538.3369 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  56 -Loss:  7918.8721 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  57 -Loss:  7139.9209 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  58 -Loss:  8625.6172 Validation Accuracy: 0.347656\n",
      "Epoch  1, Batch  59 -Loss:  8726.3301 Validation Accuracy: 0.347656\n",
      "Epoch  1, Batch  60 -Loss:  7748.0352 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  61 -Loss:  8240.5234 Validation Accuracy: 0.367188\n",
      "Epoch  1, Batch  62 -Loss:  8040.0903 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  63 -Loss:  7048.4033 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  64 -Loss:  8283.0137 Validation Accuracy: 0.371094\n",
      "Epoch  1, Batch  65 -Loss:  6548.7334 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  66 -Loss:  6070.3413 Validation Accuracy: 0.359375\n",
      "Epoch  1, Batch  67 -Loss:  6484.2476 Validation Accuracy: 0.359375\n",
      "Epoch  1, Batch  68 -Loss:  7882.0781 Validation Accuracy: 0.375000\n",
      "Epoch  1, Batch  69 -Loss:  6193.3491 Validation Accuracy: 0.386719\n",
      "Epoch  1, Batch  70 -Loss:  7603.2979 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  71 -Loss:  6469.4062 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  72 -Loss:  7095.4648 Validation Accuracy: 0.386719\n",
      "Epoch  1, Batch  73 -Loss:  9059.8740 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  74 -Loss:  5869.3755 Validation Accuracy: 0.371094\n",
      "Epoch  1, Batch  75 -Loss:  6837.3340 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  76 -Loss:  6920.2275 Validation Accuracy: 0.394531\n",
      "Epoch  1, Batch  77 -Loss:  5838.5645 Validation Accuracy: 0.390625\n",
      "Epoch  1, Batch  78 -Loss:  7160.4756 Validation Accuracy: 0.406250\n",
      "Epoch  1, Batch  79 -Loss:  6750.3154 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  80 -Loss:  6449.0488 Validation Accuracy: 0.394531\n",
      "Epoch  1, Batch  81 -Loss:  6496.3501 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  82 -Loss:  5833.5664 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  83 -Loss:  7173.1074 Validation Accuracy: 0.425781\n",
      "Epoch  1, Batch  84 -Loss:  5032.7959 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  85 -Loss:  5548.7549 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  86 -Loss:  6060.0938 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  87 -Loss:  6234.7002 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  88 -Loss:  6343.7803 Validation Accuracy: 0.421875\n",
      "Epoch  1, Batch  89 -Loss:  5038.2070 Validation Accuracy: 0.425781\n",
      "Epoch  1, Batch  90 -Loss:  5425.8984 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  91 -Loss:  4385.1714 Validation Accuracy: 0.425781\n",
      "Epoch  1, Batch  92 -Loss:  5250.5996 Validation Accuracy: 0.433594\n",
      "Epoch  1, Batch  93 -Loss:  5247.5469 Validation Accuracy: 0.433594\n",
      "Epoch  1, Batch  94 -Loss:  5994.7734 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  95 -Loss:  6265.9692 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  96 -Loss:  5787.1792 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  97 -Loss:  5367.7959 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  98 -Loss:  4324.1689 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch  99 -Loss:  5681.5117 Validation Accuracy: 0.460938\n",
      "Epoch  1, Batch 100 -Loss:  4861.1445 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch 101 -Loss:  4892.5908 Validation Accuracy: 0.460938\n",
      "Epoch  1, Batch 102 -Loss:  4481.1270 Validation Accuracy: 0.464844\n",
      "Epoch  1, Batch 103 -Loss:  5400.7764 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch 104 -Loss:  5373.1240 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch 105 -Loss:  4707.4355 Validation Accuracy: 0.464844\n",
      "Epoch  1, Batch 106 -Loss:  3446.2437 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch 107 -Loss:  3786.1338 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch 108 -Loss:  5596.2788 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch 109 -Loss:  5418.7783 Validation Accuracy: 0.484375\n",
      "Epoch  1, Batch 110 -Loss:  4816.6055 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch 111 -Loss:  4103.8320 Validation Accuracy: 0.488281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 112 -Loss:  4608.5845 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch 113 -Loss:  4930.9971 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch 114 -Loss:  3010.5059 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 115 -Loss:  3866.6904 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 116 -Loss:  4052.6040 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch 117 -Loss:  4853.0679 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch 118 -Loss:  4476.7129 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 119 -Loss:  3466.7200 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 120 -Loss:  4733.4263 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch 121 -Loss:  4124.3315 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 122 -Loss:  4780.9185 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch 123 -Loss:  4556.2539 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch 124 -Loss:  3354.7852 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch 125 -Loss:  3571.0762 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 126 -Loss:  5331.3887 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 127 -Loss:  3496.6729 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 128 -Loss:  5139.7100 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch 129 -Loss:  4321.4390 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 130 -Loss:  2582.4949 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 131 -Loss:  3795.8486 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 132 -Loss:  3794.5923 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 133 -Loss:  3832.2490 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 134 -Loss:  4080.3052 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 135 -Loss:  3990.3721 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 136 -Loss:  4223.1079 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 137 -Loss:  2905.7012 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 138 -Loss:  3539.9397 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 139 -Loss:  3898.4233 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 140 -Loss:  3273.0166 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 141 -Loss:  3867.8086 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 142 -Loss:  3927.1602 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 143 -Loss:  3586.6660 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 144 -Loss:  3386.4761 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 145 -Loss:  3120.1753 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 146 -Loss:  4611.8604 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 147 -Loss:  3824.3623 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 148 -Loss:  3555.7312 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 149 -Loss:  4233.5010 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 150 -Loss:  3612.7971 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 151 -Loss:  4415.3906 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 152 -Loss:  2839.8706 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 153 -Loss:  3247.9077 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 154 -Loss:  4107.6543 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 155 -Loss:  3554.6702 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 156 -Loss:  3835.0669 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 157 -Loss:  3865.3018 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 158 -Loss:  2642.9460 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 159 -Loss:  4192.0527 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 160 -Loss:  2975.1743 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 161 -Loss:  3621.2715 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 162 -Loss:  3169.1487 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 163 -Loss:  3958.8540 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 164 -Loss:  3746.4443 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 165 -Loss:  3615.0713 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 166 -Loss:  3045.8110 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 167 -Loss:  2771.9917 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 168 -Loss:  3683.2268 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 169 -Loss:  3535.6150 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 170 -Loss:  2913.8403 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 171 -Loss:  3327.9370 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 172 -Loss:  2725.3521 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 173 -Loss:  3291.3950 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 174 -Loss:  3098.5720 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 175 -Loss:  3837.7720 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 176 -Loss:  2708.9534 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 177 -Loss:  2985.6956 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 178 -Loss:  3177.5459 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 179 -Loss:  3001.1067 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 180 -Loss:  3246.9678 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 181 -Loss:  2617.0132 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 182 -Loss:  3752.8696 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 183 -Loss:  3998.4937 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 184 -Loss:  3208.9412 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 185 -Loss:  3170.0332 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 186 -Loss:  3212.7456 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 187 -Loss:  2567.9907 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 188 -Loss:  3755.1504 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 189 -Loss:  2704.6453 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 190 -Loss:  2974.5083 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 191 -Loss:  2020.1155 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 192 -Loss:  3023.5713 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 193 -Loss:  3227.0825 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 194 -Loss:  2885.3206 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 195 -Loss:  3461.6907 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 196 -Loss:  3565.7212 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 197 -Loss:  3416.9023 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 198 -Loss:  3342.0815 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 199 -Loss:  2921.6995 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 200 -Loss:  2603.5850 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 201 -Loss:  3510.0327 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 202 -Loss:  3248.6628 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 203 -Loss:  2409.4336 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 204 -Loss:  2637.1128 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 205 -Loss:  2979.8462 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 206 -Loss:  3133.2236 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 207 -Loss:  2805.5891 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 208 -Loss:  2429.7041 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 209 -Loss:  3110.9998 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 210 -Loss:  3407.4663 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 211 -Loss:  2465.9858 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 212 -Loss:  2463.3921 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 213 -Loss:  2663.0396 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 214 -Loss:  2996.9788 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 215 -Loss:  3026.9629 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 216 -Loss:  2118.7451 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 217 -Loss:  2746.7246 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 218 -Loss:  2893.5056 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 219 -Loss:  2335.3308 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 220 -Loss:  2868.1714 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 221 -Loss:  3874.3916 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 222 -Loss:  3434.0571 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 223 -Loss:  2638.7085 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 224 -Loss:  2443.4077 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 225 -Loss:  1896.5703 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 226 -Loss:  3123.3193 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 227 -Loss:  2699.9136 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 228 -Loss:  2907.4456 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 229 -Loss:  2778.5857 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 230 -Loss:  2710.0054 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 231 -Loss:  2967.6196 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 232 -Loss:  2614.0525 Validation Accuracy: 0.644531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 233 -Loss:  1919.3076 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 234 -Loss:  2398.5693 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 235 -Loss:  2088.5693 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 236 -Loss:  2318.1008 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 237 -Loss:  2927.4189 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 238 -Loss:  2727.3145 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 239 -Loss:  2406.1270 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 240 -Loss:  2743.8242 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 241 -Loss:  3357.7344 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 242 -Loss:  2427.9819 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 243 -Loss:  2139.3354 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 244 -Loss:  1997.3594 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 245 -Loss:  2270.8799 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 246 -Loss:  1993.4589 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 247 -Loss:  2467.5859 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 248 -Loss:  2369.0088 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 249 -Loss:  2760.6519 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 250 -Loss:  2042.0198 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 251 -Loss:  2128.1335 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 252 -Loss:  2545.9021 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 253 -Loss:  2599.3813 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 254 -Loss:  1706.1793 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 255 -Loss:  2097.0242 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 256 -Loss:  2303.4204 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 257 -Loss:  1861.5509 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 258 -Loss:  1425.0581 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 259 -Loss:  2322.9761 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 260 -Loss:  2327.3501 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 261 -Loss:  1764.7777 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 262 -Loss:  2918.6064 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 263 -Loss:  2159.2729 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 264 -Loss:  2053.6824 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 265 -Loss:  2029.8505 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 266 -Loss:  1524.1460 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 267 -Loss:  2776.0872 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 268 -Loss:  1870.1221 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 269 -Loss:  2087.3545 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 270 -Loss:  2139.9788 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 271 -Loss:  2526.8101 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 272 -Loss:  2185.7290 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 273 -Loss:  1902.8193 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 274 -Loss:  2612.9956 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 275 -Loss:  2489.7603 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 276 -Loss:  2282.2949 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 277 -Loss:  2105.5027 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 278 -Loss:  2289.3745 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 279 -Loss:  1795.5403 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 280 -Loss:  1788.7457 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 281 -Loss:  2437.5044 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 282 -Loss:  2436.9360 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 283 -Loss:  2872.3098 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 284 -Loss:  2526.2952 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 285 -Loss:  2198.3293 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 286 -Loss:  2064.2402 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 287 -Loss:  1752.1166 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 288 -Loss:  2153.7993 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 289 -Loss:  2179.0923 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 290 -Loss:  2064.3435 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 291 -Loss:  2225.1147 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 292 -Loss:  1941.3888 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 293 -Loss:  2794.2134 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 294 -Loss:  2513.3389 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 295 -Loss:  2367.4763 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 296 -Loss:  2087.1343 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 297 -Loss:  2026.8658 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 298 -Loss:  2387.1519 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 299 -Loss:  1998.6023 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 300 -Loss:  1745.9722 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 301 -Loss:  1508.5520 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 302 -Loss:  1657.8263 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 303 -Loss:  1994.8401 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 304 -Loss:  2851.7725 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 305 -Loss:  1965.4052 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 306 -Loss:  2579.3604 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 307 -Loss:  1861.9805 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 308 -Loss:  1661.0320 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 309 -Loss:  1679.3420 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 310 -Loss:  2101.1719 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 311 -Loss:  1933.7887 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 312 -Loss:  1932.3202 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 313 -Loss:  1367.3462 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 314 -Loss:  1702.0476 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 315 -Loss:  1998.2997 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 316 -Loss:  1875.9426 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 317 -Loss:  1889.4159 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 318 -Loss:  1866.3702 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 319 -Loss:  1502.5806 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 320 -Loss:  1485.6459 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 321 -Loss:  2390.3674 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 322 -Loss:  1388.8943 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 323 -Loss:  1835.0133 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 324 -Loss:  1899.7039 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 325 -Loss:  1861.1624 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 326 -Loss:  1985.5634 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 327 -Loss:  1518.2078 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 328 -Loss:  2369.4966 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 329 -Loss:  1943.8071 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 330 -Loss:  1508.7972 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 331 -Loss:  1932.7401 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 332 -Loss:  1831.3743 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 333 -Loss:  1633.0197 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 334 -Loss:  1866.3896 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 335 -Loss:  2016.1851 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 336 -Loss:  1603.6033 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 337 -Loss:  1203.9667 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 338 -Loss:  2197.8152 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 339 -Loss:  2254.6816 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 340 -Loss:  2096.7161 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 341 -Loss:  2344.3562 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 342 -Loss:  1902.1676 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 343 -Loss:  1992.7554 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 344 -Loss:  2352.6763 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 345 -Loss:  1798.8445 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 346 -Loss:  1342.9844 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 347 -Loss:  1988.8499 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 348 -Loss:  1815.5195 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 349 -Loss:  1804.6130 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 350 -Loss:  1410.3270 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 351 -Loss:  1635.1985 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 352 -Loss:  1950.1311 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 353 -Loss:  1577.8580 Validation Accuracy: 0.675781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 354 -Loss:  1615.6357 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 355 -Loss:  1775.6987 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 356 -Loss:  1340.7915 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 357 -Loss:  1311.0804 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 358 -Loss:  2165.3955 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 359 -Loss:  2388.9224 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 360 -Loss:  1518.4995 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 361 -Loss:  1559.8733 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 362 -Loss:  1746.3530 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 363 -Loss:  1637.4587 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 364 -Loss:  1321.5415 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 365 -Loss:  2028.3999 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 366 -Loss:  1859.9236 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 367 -Loss:  1576.3666 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 368 -Loss:  1462.2288 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 369 -Loss:  1751.8268 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 370 -Loss:  1327.4963 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 371 -Loss:  1923.5483 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 372 -Loss:   990.9591 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 373 -Loss:  1801.3528 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 374 -Loss:  1804.5078 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 375 -Loss:  1386.9282 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 376 -Loss:  1580.8461 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 377 -Loss:  1764.5762 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 378 -Loss:  1558.9692 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 379 -Loss:  1465.7518 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 380 -Loss:  1641.6458 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 381 -Loss:  1549.1462 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 382 -Loss:  1422.2295 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 383 -Loss:  1553.3535 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 384 -Loss:  1713.2581 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 385 -Loss:  1935.7275 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 386 -Loss:  1852.7466 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 387 -Loss:  2110.6653 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 388 -Loss:  1450.3027 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 389 -Loss:  1758.3015 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 390 -Loss:  1876.2742 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 391 -Loss:  1487.1091 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 392 -Loss:  2230.6733 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 393 -Loss:  1519.9591 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 394 -Loss:  1331.3862 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 395 -Loss:  2126.9014 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 396 -Loss:  1668.4120 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 397 -Loss:  1561.2675 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 398 -Loss:  1496.2573 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 399 -Loss:  1768.5677 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 400 -Loss:  2197.2856 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 401 -Loss:  1322.0485 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 402 -Loss:  1345.2505 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 403 -Loss:  2293.6421 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 404 -Loss:  1594.9062 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 405 -Loss:  1346.7727 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 406 -Loss:  1259.3904 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 407 -Loss:  1842.2595 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 408 -Loss:  1539.0920 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 409 -Loss:  1325.3884 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 410 -Loss:  1483.6292 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 411 -Loss:  2050.8840 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 412 -Loss:  1597.0454 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 413 -Loss:  1091.2119 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 414 -Loss:  1802.3635 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 415 -Loss:  1332.8975 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 416 -Loss:  2207.1187 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 417 -Loss:  1441.3236 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 418 -Loss:  1426.9958 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 419 -Loss:  1779.5118 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 420 -Loss:  1790.1235 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 421 -Loss:  1547.6245 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 422 -Loss:  1670.3545 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 423 -Loss:  1583.1160 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 424 -Loss:  1143.1527 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 425 -Loss:  1572.7683 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 426 -Loss:  1535.7820 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 427 -Loss:  1461.6559 Validation Accuracy: 0.703125\n"
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can further improve the accuracy by increasing the epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
